{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the text splitter and splitting the document\n",
    "\n",
    "gpt4all1.pdf can be accessed in their github repo, first paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.gpt4all import GPT4AllEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    "    # separators='.'\n",
    ")\n",
    "\n",
    "# loader = TextLoader(\"mlops.txt\")\n",
    "loader = PyPDFLoader(\"gpt4all1.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "documents = loader.load()\n",
    "# nltk_splitter = NLTKTextSplitter()\n",
    "texts = text_splitter.split_documents(documents)\n",
    "# texts = nltk_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the splitted text into the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/random/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[1795]: Class GGMLMetalClass is implemented in both /Users/random/miniconda3/envs/llm/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x12fb14228) and /Users/random/miniconda3/envs/llm/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x12f9dc228). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = GPT4AllEmbeddings()\n",
    "db = FAISS.from_documents(texts, embedding=embedder)\n",
    "# db = FAISS.from_embeddings\n",
    "# db.save_local('db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is gpt4all\"\n",
    "docs = db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(docs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpage_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info(query):\n",
    "    similar_response = db.similarity_search(query, k=3)\n",
    "\n",
    "    page_contents_array = [doc.page_content for doc in similar_response]\n",
    "\n",
    "    # print(page_contents_array)\n",
    "\n",
    "    return page_contents_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/random/.cache/gpt4all/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/random/.cache/gpt4all/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7477.73 MB (+ 1600.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/random/miniconda3/envs/llm/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x16647c370\n",
      "ggml_metal_init: loaded kernel_add_row                        0x166482350\n",
      "ggml_metal_init: loaded kernel_mul                            0x166481960\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x166481bc0\n",
      "ggml_metal_init: loaded kernel_scale                          0x166483140\n",
      "ggml_metal_init: loaded kernel_silu                           0x166483800\n",
      "ggml_metal_init: loaded kernel_relu                           0x16647b370\n",
      "ggml_metal_init: loaded kernel_gelu                           0x166520830\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x166520fc0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x166521f60\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x166522a60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x166523a20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x166523080\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x16256dbe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x16256ea70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x16256f0f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1664846d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x166484930\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1664852c0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1664859f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x166486ae0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x166487270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x166487a20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1665217c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x166524530\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x16256e550\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x16256fad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1625706f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x16256feb0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x162571640\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1625722d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x166525510\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x166525ae0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 6996.69 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1602.00 MB, ( 8598.69 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   290.00 MB, ( 8888.69 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 9080.69 / 10922.67)\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "local_path = (\n",
    "    # \"/Users/random/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\"  # replace with your desired local file path\n",
    "    \"/Users/random/.cache/gpt4all/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\"  # replace with your desired local file path\n",
    ")\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=local_path, backend=\"llama\", verbose=True, streaming=True, callbacks=callbacks, max_tokens=16000\n",
    ")\n",
    "\n",
    "# template = \"\"\"You are a helpful AI assistant and provide the answer for the question based on the given context and your existing knowledge.\n",
    "# Context:{context}\n",
    "# >>QUESTION<<{message}\n",
    "# >>ANSWER<<\"\"\"\n",
    "\n",
    "template = \"\"\"You are a helpful AI assistant and provide the answer for the question based on the given context and your existing knowledge.\n",
    "Context:{context}\n",
    ">>QUESTION<<{message}\n",
    ">>ANSWER<<\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"message\"], template=template)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(message):\n",
    "    context = retrieve_info(message)\n",
    "    # print(context)\n",
    "    response = llm_chain.run(message=message, context=context)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4All is an open source, large scale chatbot trained over a curated corpus of assistant interactions. The project aims to promote reproducibility and collaboration in AI research by releasing data, training code, and final model weights for the community to build upon. GPT4All was developed using LLaMA, which has a non-commercial license, and based on Ope"
     ]
    }
   ],
   "source": [
    "# actual GPU inference!!!!\n",
    "question = \"gpt4all\"\n",
    "result = generate_response(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we comment the context part from the ```generate_response``` function, we will see that the model starts hallucinating as it has no idea what gpt4all is. But after putting the correct context from the database, we can see that the model generated nice results and can clearly summarize what's going on in the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
