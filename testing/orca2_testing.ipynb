{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2820c2-dd95-403e-bc39-18c3ca4259eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T11:57:10.287610Z",
     "iopub.status.busy": "2023-11-28T11:57:10.287208Z",
     "iopub.status.idle": "2023-11-28T11:57:25.858089Z",
     "shell.execute_reply": "2023-11-28T11:57:25.857292Z",
     "shell.execute_reply.started": "2023-11-28T11:57:10.287586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, torch, logging\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ad56ee-ee33-4862-aa4a-a0c62b3e5e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T11:57:48.727320Z",
     "iopub.status.busy": "2023-11-28T11:57:48.726969Z",
     "iopub.status.idle": "2023-11-28T11:57:48.752814Z",
     "shell.execute_reply": "2023-11-28T11:57:48.752293Z",
     "shell.execute_reply.started": "2023-11-28T11:57:48.727297Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the key challenge with full fine-tunin...</td>\n",
       "      <td>Full fine-tuning of large models like GPT-3, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Low-Rank Adaptation (LoRA)?</td>\n",
       "      <td>LoRA is a method that freezes the pre-trained ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does LoRA compare to full fine-tuning in t...</td>\n",
       "      <td>LoRA can reduce the number of trainable parame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the impact of LoRA on inference latency?</td>\n",
       "      <td>LoRA introduces no additional inference latenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can LoRA be combined with other adaptation met...</td>\n",
       "      <td>Yes, LoRA is orthogonal to many prior methods ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question                                             Answer\n",
       "0  What is the key challenge with full fine-tunin...  Full fine-tuning of large models like GPT-3, w...\n",
       "1                What is Low-Rank Adaptation (LoRA)?  LoRA is a method that freezes the pre-trained ...\n",
       "2  How does LoRA compare to full fine-tuning in t...  LoRA can reduce the number of trainable parame...\n",
       "3   What is the impact of LoRA on inference latency?  LoRA introduces no additional inference latenc...\n",
       "4  Can LoRA be combined with other adaptation met...  Yes, LoRA is orthogonal to many prior methods ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('final_data1.tsv', sep=\"\\t\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e111cb-cb31-4d85-b451-c0481e9b5552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T11:57:51.053249Z",
     "iopub.status.busy": "2023-11-28T11:57:51.052930Z",
     "iopub.status.idle": "2023-11-28T11:57:51.083295Z",
     "shell.execute_reply": "2023-11-28T11:57:51.082773Z",
     "shell.execute_reply.started": "2023-11-28T11:57:51.053229Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"<s>[INST] Answer the following question: \"\n",
    "data[\"text\"] = (\n",
    "    instruction + data[\"Question\"] + \"[/INST] \" + data[\"Answer\"] + \" </s>\"\n",
    ")\n",
    "\n",
    "# Drop other columns so that only the 'text' column remains\n",
    "data = data[[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b41b4c9-836c-4625-b511-073ac611872b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T11:57:53.827036Z",
     "iopub.status.busy": "2023-11-28T11:57:53.826728Z",
     "iopub.status.idle": "2023-11-28T11:57:53.862886Z",
     "shell.execute_reply": "2023-11-28T11:57:53.862268Z",
     "shell.execute_reply.started": "2023-11-28T11:57:53.827018Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "training_data = Dataset(pa.Table.from_pandas(data.reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37d64c1-9ce6-4e42-a792-eaa35c03c91a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T11:57:56.284311Z",
     "iopub.status.busy": "2023-11-28T11:57:56.283656Z",
     "iopub.status.idle": "2023-11-28T12:05:57.552968Z",
     "shell.execute_reply": "2023-11-28T12:05:57.552167Z",
     "shell.execute_reply.started": "2023-11-28T11:57:56.284288Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d768e365794d9f94547c1e017e56c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/828 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2576397e69124732984c5e8b11f7791f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dceb8c470084e29b9d793e4ffc468cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954135ebc1dd41ffa2d9fd4f054a742f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697ec26201744f15af2585b75db3b50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595ec25d3bf94ae3b51ca86f6c8f3835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8823630573cd477485f5bb2877fd2f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33dd9797efa413580d8f3ce31e93f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1a8d17aecf41e88698a357fab1afb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e632eb1241244ed9b3b77c87eee55e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5ce33b9a634555811cb21db48e5fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2189947f65647f8bbc038cb35454cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"microsoft/Orca-2-7b\"\n",
    "refined_model = \"orca2-7b-neuralearn-qlora-ft\"\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "quant_8bits = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171e9bca-6c1b-4f07-b2ef-74eaad8be105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T12:06:05.704396Z",
     "iopub.status.busy": "2023-11-28T12:06:05.703740Z",
     "iopub.status.idle": "2023-11-28T12:06:05.886993Z",
     "shell.execute_reply": "2023-11-28T12:06:05.886430Z",
     "shell.execute_reply.started": "2023-11-28T12:06:05.704362Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:173: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dff25f9e554c7cad75c6a09941d183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_neuralearn_orca\",\n",
    "    num_train_epochs=5,\n",
    "    save_steps=50,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=2,\n",
    "    logging_steps=20,\n",
    "    fp16=True,\n",
    "    seed=42,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b284e890-eebb-4665-8438-b6e5f23f3f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T12:06:08.023821Z",
     "iopub.status.busy": "2023-11-28T12:06:08.023369Z",
     "iopub.status.idle": "2023-11-28T12:12:26.908397Z",
     "shell.execute_reply": "2023-11-28T12:12:26.907561Z",
     "shell.execute_reply.started": "2023-11-28T12:06:08.023798Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 06:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.631300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.970700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.912900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=285, training_loss=1.1848941384700307, metrics={'train_runtime': 378.6944, 'train_samples_per_second': 5.994, 'train_steps_per_second': 0.753, 'total_flos': 2.172347346419712e+16, 'train_loss': 1.1848941384700307, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376612c9-5e3b-4100-b9e9-3299c81155ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T12:16:33.647472Z",
     "iopub.status.busy": "2023-11-28T12:16:33.647002Z",
     "iopub.status.idle": "2023-11-28T12:16:33.848735Z",
     "shell.execute_reply": "2023-11-28T12:16:33.848142Z",
     "shell.execute_reply.started": "2023-11-28T12:16:33.647442Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "fine_tuning.save_model('orca2-ft-neuralearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ca5fcd5-4779-492b-a3b9-4354e007a9a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T12:16:35.221044Z",
     "iopub.status.busy": "2023-11-28T12:16:35.220557Z",
     "iopub.status.idle": "2023-11-28T12:16:53.806333Z",
     "shell.execute_reply": "2023-11-28T12:16:53.805616Z",
     "shell.execute_reply.started": "2023-11-28T12:16:35.221017Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d38bbc39694c3da43eaf2827cb733b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "output_dir = \"orca2-ft-neuralearn\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84e2cd9-64f9-4b2c-95b5-735e65b5d9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T12:19:00.553863Z",
     "iopub.status.busy": "2023-11-28T12:19:00.553513Z",
     "iopub.status.idle": "2023-11-28T12:19:31.916110Z",
     "shell.execute_reply": "2023-11-28T12:19:31.915442Z",
     "shell.execute_reply.started": "2023-11-28T12:19:00.553840Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "        You are a helpful assistant designed to help people study machine learning. Your answers are always brief.\n",
      "        <</SYS>>\" What is qlora in machine learning? \"[/INST] A QLORA is a type of neural network whose weights are updated using QLORA optimization algorithm.  It is a new type of  neural network that was invented by Google AI research team.  The QLORA algorithm is designed to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is a new optimization method for training neural networks that uses a new way to update the weights of the network.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is a new optimization method for training neural networks that uses a new way to update the weights of the network.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way.  The QLORA algorithm is an optimization method that aims to improve the performance of neural networks by adjusting the weights in a more efficient way\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is qlora in machine learning?\"\n",
    "input_prompt = f\"\"\"[INST]<<SYS>>\n",
    "        You are a helpful assistant designed to help people study machine learning. Your answers are always brief.\n",
    "        <</SYS>>\" {prompt} \"[/INST]\"\"\"\n",
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.5)\n",
    "\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
